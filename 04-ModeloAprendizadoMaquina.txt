### Projeto feito no google colab ###
##Arquivo Projeto_Spark_2.ipynb

#  Contextualização do problema

O dataset de vinho contém 178 amostras de vinho com 13 características físico-químicas, como a quantidade de álcool, acidez, entre outros, e uma classificação da qualidade em três categorias: 0, 1 ou 2.

Nosso objetivo é prever a qualidade do vinho utilizando essas 13 características, ou seja, é um problema de classificação multiclasse. Vamos começar com o modelo de Regressão Logística, que é adequado para tarefas de classificação.

#  Carregando os dados e transformando em DataFrame

!pip install pyspark

from sklearn.datasets import load_wine
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, abs as spark_abs
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import pandas as pd

# Iniciar a sessão do Spark
spark = SparkSession.builder.appName("WineDataset").getOrCreate()

# Carregar o dataset Wine
wine_data = load_wine()

wine_df = pd.DataFrame(data=wine_data.data, columns=wine_data.feature_names)

# Adicionar coluna Qualidade
wine_df['quality'] = wine_data.target

# Display the distinct values of the quality column
wine_df.head()

# Converter para um DataFrame do Spark
spark_df = spark.createDataFrame(wine_df)

# Verificar a estrutura dos dados
print(spark.sparkContext.appName)
spark_df.show(5)


#  Tratamento dos dados

center_stat = 'median'

# Definir as colunas de características
all_cols = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash']

# Montar o vetor de características
assembler = VectorAssembler(inputCols=all_cols, outputCol="features")

# Aplicar o assembler ao DataFrame
df_features = assembler.transform(spark_df)

# Normalizar as características
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
scaler_model = scaler.fit(df_features)
df_scaled = scaler_model.transform(df_features)

# Selecionar apenas a coluna normalizada e a variável target (quality)
final_df = df_scaled.select("scaledFeatures", "quality")
final_df.show(5)


#  Dividindo os dados em treino e teste

# Divisão dos dados em 80% treino e 20% teste
train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)


#  Treinando o modelo de Regressão Logística

# Inicializando o modelo de regressão logística
logreg = LogisticRegression(featuresCol="scaledFeatures", labelCol="quality")

# Treinando o modelo
logreg_model = logreg.fit(train_df)

# Avaliando o modelo no conjunto de teste
predictions = logreg_model.transform(test_df)

# Avaliando a acurácia
evaluator = MulticlassClassificationEvaluator(labelCol="quality", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

print(f"Acurácia no conjunto de teste (Regressão Logística): {accuracy:.2f}")


#  Avaliação de Métricas e Matriz de Confusão

# Gerar previsões no conjunto de teste
predictions.select("quality", "prediction").show(10)

# Calcular a matriz de confusão
predictions.groupBy("quality", "prediction").count().show()

#  Opções de Melhoria 

Podemos optar pelo modelo de Random Forest que em sua maioria das vezes é superior, podemos optar por ele para capturar melhor as interações entre as variáveis. 
No entanto, outras abordagens como ajuste de hiperparâmetros e diferentes métodos de validação cruzada podem ser testadas para otimizar ainda mais os resultados.
